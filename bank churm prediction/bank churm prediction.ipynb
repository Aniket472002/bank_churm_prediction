{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MAUa-rerdP3F"
   },
   "source": [
    "# **Bank Customer Churn Prediction Project**\n",
    "\n",
    "- **Note:** \"Feedback from phase 1 was taken into consideration as much as possible while working on this notebook.\"\n",
    "\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "My name: Mostapha Abdulaziz\n",
    "\n",
    "## Overview\n",
    "This project focuses on predicting customer churn in the banking sector using machine learning. The workflow involves exploratory data analysis, clustering, building and comparing individual and ensemble models, handling imbalanced data, and evaluating results using robust metrics.\n",
    "\n",
    "## Objectives\n",
    "1. Understand the dataset and its features.\n",
    "2. Utilize clustering for data preparation.\n",
    "3. Train individual machine learning models and ensemble methods.\n",
    "4. Handle class imbalance using various techniques and evaluate their impact.\n",
    "5. Analyze results to identify the best-performing model and interpret its bias/variance.\n",
    "\n",
    "## Workflow\n",
    "1. **Data Exploration and Preprocessing**:\n",
    "   - Handle missing values, outliers, and feature encoding.\n",
    "2. **Clustering**:\n",
    "   - Apply clustering algorithms for customer segmentation.\n",
    "3. **Modeling**:\n",
    "   - Train models like Logistic Regression, SVM, and Random Forest.\n",
    "   - Build ensemble methods like Bagging, Boosting, and Stacking.\n",
    "4. **Imbalanced Data Handling**:\n",
    "   - Use methods like SMOTE, undersampling, and class weighting.\n",
    "5. **Evaluation**:\n",
    "   - Compare models using accuracy, precision, recall, F1-score, confusion matrix, and ROC-AUC.\n",
    "6. **Documentation**:\n",
    "   - Provide insights and conclusions in a detailed report (Submitted seperatly).\n",
    "\n",
    "## Key Features\n",
    "- **Dataset**: Bank customer data with attributes like credit score, balance, tenure, and churn status.\n",
    "- **Algorithms**: Logistic Regression, SVM, Random Forest, and ensemble techniques.\n",
    "- **Evaluation**: ROC curves, confusion matrix, and other metrics.\n",
    "\n",
    "## Prerequisites\n",
    "- Python.\n",
    "- Libraries: pandas, numpy, matplotlib, seaborn, scikit-learn.\n",
    "\n",
    "## Outputs\n",
    "- Model performance comparisons.\n",
    "- Insights on the impact of class imbalance handling techniques.\n",
    "\n",
    "Instructions\n",
    "1. Clone the repository and navigate to the project folder:\n",
    "   ```bash\n",
    "   git clone https://github.com/mostapha227824/Bank_customer_churn_prediction.git\n",
    "   cd Bank_customer_churn_prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uRM1ogT0geez"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2h1B82ZY4D_"
   },
   "source": [
    "# 1. **Describing and preprocessing the dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UgCqwBa4e2Ud"
   },
   "source": [
    "# Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pj_CzkaGdPF2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e1pvMywigfy6"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xt0DkFjJdGz2"
   },
   "source": [
    "# Exploration and preprocessing step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tPct_txsdHK0",
    "outputId": "cacc21ea-00de-42d2-c193-66d3e7f1d782"
   },
   "outputs": [],
   "source": [
    "data_path = \"/kaggle/input/bank-customer-churn-prediction/Churn_Modelling.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NG411bC6fSrD"
   },
   "source": [
    "# Summary of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cmf6cUesfS79",
    "outputId": "ae2846e4-9d6e-4c74-ce93-8f38a34e3127"
   },
   "outputs": [],
   "source": [
    "print(\"\\nDataset Info:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6d0NJOZPgiQr"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pK15lU37feD6"
   },
   "source": [
    "# Checking for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kZzhfJNqfeUj",
    "outputId": "a2bc0086-3850-4f88-cf0a-de9270771788"
   },
   "outputs": [],
   "source": [
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O1S-E0V-f2w5"
   },
   "source": [
    "**It seems that theres Only One null values in 'Geography', 'Age', HasCrCard' and 'IsActiveMember' columns.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v1oFNi_Hf6yu",
    "outputId": "dc05a075-9098-4238-e3c2-712242160d0a"
   },
   "outputs": [],
   "source": [
    "df[df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EWgZIFC0gDJV"
   },
   "source": [
    "- Since there are only 4 rows with null values in the entire dataset, I will drop these rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2oswaYrQgMDW"
   },
   "source": [
    "# dropping rows with null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gtRLjxlPgAMi",
    "outputId": "11584aba-4c78-429e-8f99-278f7707b48d"
   },
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62ntdDi7gj4X"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8vANyvIfjej"
   },
   "source": [
    "# Checking for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "14azndoEfjzo",
    "outputId": "ae6b33e2-3497-49bc-cd9d-7f1349266fd9"
   },
   "outputs": [],
   "source": [
    "print(\"\\nDuplicate Rows:\", df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0wBTN0aEgUp3"
   },
   "source": [
    "# dropping duplicated row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hPbfMBo3gVQe",
    "outputId": "b67eb484-e640-4bac-fdd8-7cc68612c372"
   },
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)\n",
    "print(\"\\nDuplicate Rows:\", df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJ_tP8toglai"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w7ORJTVqgzKM"
   },
   "source": [
    "# data shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a9Hpjm00gzbp",
    "outputId": "dad4bbd3-7938-400d-a3ed-76d34b76ce76"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mwdrj0MQg-gO"
   },
   "source": [
    "# checking for unique values of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Su1myKYSg-z1"
   },
   "outputs": [],
   "source": [
    "unique_values_df = pd.DataFrame({\n",
    "    'Column': df.columns,\n",
    "    'Unique Values': [df[col].unique() for col in df.columns],\n",
    "    'No. of Unique Values': [df[col].nunique() for col in df.columns]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kV-LgK4GjwWT"
   },
   "source": [
    "# Feature types Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LPmaawcNjqYB"
   },
   "outputs": [],
   "source": [
    "numerical_features = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'EstimatedSalary']\n",
    "categorical_features = ['Geography', 'Gender', 'HasCrCard', 'IsActiveMember']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L84YEZTchItC"
   },
   "source": [
    "# Displaing the DataFrame in a well-formatted way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J5yF613FhJAF",
    "outputId": "290b8d96-468f-4984-ba39-e2ebd8cc60af"
   },
   "outputs": [],
   "source": [
    "unique_values_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qmDim7iXjztc"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CkwlOrBqjgti"
   },
   "source": [
    "# Detectting outliers using boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lh6Xfc3ynT-_",
    "outputId": "bae076ad-c10f-4202-805d-ec16ebdd4b18"
   },
   "outputs": [],
   "source": [
    "num_cols = df.select_dtypes(include=['number']).columns\n",
    "\n",
    "# Columns to explore\n",
    "to_explore = [col for col in num_cols if col not in ['Exited']]\n",
    "\n",
    "for column in to_explore:\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Identifing outliers\n",
    "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "    outlier_count = len(outliers)\n",
    "\n",
    "    fig = make_subplots(rows=1, cols=2, subplot_titles=(f'{column} - Boxplot', f'{column} - Distplot'))\n",
    "\n",
    "    # Boxplot\n",
    "    fig.add_trace(go.Box(\n",
    "        y=df[column],\n",
    "        name='Boxplot',\n",
    "        boxmean='sd'\n",
    "    ), row=1, col=1)\n",
    "\n",
    "    # Distplot (Density Plot)\n",
    "    fig.add_trace(go.Histogram(\n",
    "        x=df[column],\n",
    "        name='Distplot',\n",
    "        histnorm='probability density',\n",
    "        nbinsx=30,\n",
    "        opacity=0.6,\n",
    "        marker_color='skyblue'\n",
    "    ), row=1, col=2)\n",
    "\n",
    "    # Add annotations to Boxplot\n",
    "    fig.add_annotation(\n",
    "        x=1,  # Center of the boxplot\n",
    "        y=df[column].max(),\n",
    "        text=f'No Outliers: {outlier_count}',\n",
    "        showarrow=False,\n",
    "        arrowhead=2,\n",
    "        ax=0,\n",
    "        ay=-50,\n",
    "        xref='x1',\n",
    "        yref='y1'\n",
    "    )\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=f'Interactive Plot for {column}',\n",
    "        xaxis_title='Values',\n",
    "        yaxis_title='Density',\n",
    "        xaxis2_title='Values',\n",
    "        yaxis2_title='Density',\n",
    "        height=600,\n",
    "        width=1000,\n",
    "        showlegend=False\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q3nXRS6Vh7P5",
    "outputId": "ef043b5f-073b-4c09-8043-341945bd28bf"
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pGliWzcpiAQw"
   },
   "source": [
    "**Observations:**\n",
    "\n",
    "- Based on the data description, there are no obvious outliers in the dataset's columns.\n",
    "\n",
    "- As, the CreditScore values does not exhibit significant outliers, as they are not far from the overall distribution therefore no action is needed.\n",
    "\n",
    "- In the NumOfProducts column, a value of 4 is not considered an outlier despite its low frequency.\n",
    "\n",
    "- For the Age column, since the class from 70 to 100 has minimal contribution to the dataset, any age value greater than 70 will be capped at 70.\n",
    "\n",
    "- One notable observation is the imbalance in the Exited column, which indicates the target variable. This imbalance will be further investigated and visualized to assess its impact on the model's performance and determine what resampling techniques needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZ9aGtm9o3fC"
   },
   "source": [
    "# assignning value of 70 to any higher 'Age' value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aDCBiOUSo39_"
   },
   "outputs": [],
   "source": [
    "df.loc[df['Age'] > 70, 'Age'] = 70"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "naMKeWXYkEtp"
   },
   "source": [
    "# Saving the processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rYO4yv_ukKFo",
    "outputId": "aba20daa-d0ef-44c5-ddba-f08f5890acbf"
   },
   "outputs": [],
   "source": [
    "processed_data_path = \"processed_bank_churn.csv\"\n",
    "df.to_csv(processed_data_path, index=False)\n",
    "print(f\"Processed data saved to {processed_data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B83pe6K7kbnz"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ueAe-PQiVo_"
   },
   "source": [
    "# **Visualizations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BtlfCIZXiV2x",
    "outputId": "c15e680d-484f-46cb-87bf-58eb5d6a47db"
   },
   "outputs": [],
   "source": [
    "df.hist(figsize=(15, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IEA_NyTjkfQk"
   },
   "source": [
    "**Key Observations from Data Histograms:**\n",
    "\n",
    "- Two clear observations emerged from the data histogram plots:\n",
    "\n",
    "1. 'Balance' Column: theres many zeros which stands out as unusual. This was already noted during the review of the first 10 rows of data. i will investigate more to see why this happen and how it correlates the other features,\n",
    "\n",
    "2. 'Exited' Column Imbalance: The target variable, Exited, shows a significant class imbalance. Around 80% of the data represents customers who stayed (1), while only 20% exited (0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Md5xHgYblJLv"
   },
   "source": [
    "dorpping Surname column will be dropped from the dataset before modeling as it has no potentional need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E3KXD9O0lKgI",
    "outputId": "a8223f61-d3b2-44a1-f9ff-330e4e1dc605"
   },
   "outputs": [],
   "source": [
    "df.drop('Surname', axis=1, inplace=True)\n",
    "print(\"Updated Columns:\")\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5cCsV8JPlmIW"
   },
   "source": [
    "# countplot for 'Geography'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8LBWoDPWlfIN",
    "outputId": "54d018e8-64c1-4af1-c7ea-5193156ce3a2"
   },
   "outputs": [],
   "source": [
    "geography_labels = ['France', 'Spain', 'Germany']\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=df, x='Geography', hue='Exited')\n",
    "plt.xticks(ticks=[0, 1, 2], labels=geography_labels)\n",
    "plt.xlabel('Geography')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Count of Exited Customers by Geography')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QW9LO60Fmb1A"
   },
   "source": [
    "- **The Geography countplot highlights a noticeable imbalance in the Exited column across different regions. This imbalance is particularly in France and Spain, where a significant number of customers did not exit while germany shows a slightly more balanced distribution these regional differences in customer behavior may provide valuable insights for the model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2w7zdKiApz_2"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eVAGvq06ppzh"
   },
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TgUQinaHp184",
    "outputId": "b1ecddc7-a837-4bbe-a912-563cf2057821"
   },
   "outputs": [],
   "source": [
    "zero_balance_df = df[df['Balance'] == 0].groupby(['IsActiveMember', 'Exited']).size().reset_index(name='Count')\n",
    "zero_balance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BKLavKaQvMap"
   },
   "source": [
    "This shows a conflict. How a member to Exit and still being an active member (180 member).\n",
    "\n",
    "So, i'll change these members to be inactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YG0LD5cep_vH"
   },
   "outputs": [],
   "source": [
    "# change 0 Balance and 1 Exited members IsActiveMember to 0\n",
    "df.loc[(df['Balance'] == 0) & (df['Exited'] == 1) & (df['IsActiveMember'] == 1), 'IsActiveMember'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AvB4jx-yqEPH",
    "outputId": "a3c15207-afa1-47ca-87eb-8057f461f950"
   },
   "outputs": [],
   "source": [
    "# group df according to NumOfProducts, HasCrCard, IsActiveMember\n",
    "active_level_df = df.groupby([ 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'Exited']).size()\n",
    "active_level_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aUeN9T8jqJKv"
   },
   "source": [
    "It seems weired that some customers that exited the bank still recorded as active members and have credit card.\n",
    "\n",
    "I'll change any exited member to inactive and has no credit card."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zjUaGtdOqK-o"
   },
   "outputs": [],
   "source": [
    "# change any exited member to IsActiveMember 0 and HasCrCard 0\n",
    "df.loc[(df['Exited'] == 1) & (df['IsActiveMember'] == 1), 'IsActiveMember'] = 0\n",
    "df.loc[(df['Exited'] == 1) & (df['HasCrCard'] == 1), 'HasCrCard'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7L6ApQPsYQF"
   },
   "source": [
    "# Feature Creation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pz55V5zOsYkG"
   },
   "outputs": [],
   "source": [
    "# total_active column\n",
    "df['total_active'] = df['IsActiveMember'] + df['NumOfProducts'] + df['HasCrCard']\n",
    "\n",
    "#  balance to salary\n",
    "df['Balance_to_Salary'] = df['Balance'] / df['EstimatedSalary']\n",
    "\n",
    "# tenure to age column\n",
    "df['Tenure_to_Age'] = df['Tenure'] / df['Age']\n",
    "\n",
    "# balance age interaction\n",
    "df['Balance_Age_Interaction'] = df['Balance'] * df['Age']\n",
    "\n",
    "# product age interaction\n",
    "df['Products_Age_Interaction'] = df['NumOfProducts'] * df['Age']\n",
    "\n",
    "# balance age ratio\n",
    "df['Balance_to_Age'] = df['Balance'] / df['Age']\n",
    "\n",
    "# balance product ratio\n",
    "df['Balance_to_Products'] = df['Balance'] / df['NumOfProducts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u_lYfZSYvee5",
    "outputId": "22c5ed6e-7396-48b4-dae6-2f08659d7314"
   },
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c1OWIqR7vobp"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aoo0QAfzvnnP"
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BCRLbPr_vtpq"
   },
   "source": [
    "- In this section, i will perform the following tasks:\n",
    "\n",
    "1. Encoding categorical features as needed.\n",
    "2. Splitting data.\n",
    "3. Normalizing features to ensure they are on a similar scale.-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8zACyQ5lvz0w"
   },
   "source": [
    "Since all categorical features are nominal and do not have any inherent order, I will use One-Hot Encoding to handle them appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xovyLcgFv64y"
   },
   "source": [
    "# encoding categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_iv4gO-Pvpey"
   },
   "outputs": [],
   "source": [
    "cat_cols = ['Geography', 'Gender']\n",
    "bank_df_encoded = pd.get_dummies(df, columns=cat_cols, drop_first=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yQ44dQe8v_RQ"
   },
   "source": [
    "# transforming bool columns to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v92DpJm6v_kD"
   },
   "outputs": [],
   "source": [
    "bool_cols = bank_df_encoded.select_dtypes(include=['bool']).columns\n",
    "bank_df_encoded[bool_cols] = bank_df_encoded[bool_cols].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hCiPy0bkwDki"
   },
   "source": [
    "# Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LVdWClxAwD4k"
   },
   "outputs": [],
   "source": [
    "X = bank_df_encoded.drop('Exited', axis=1)\n",
    "y = bank_df_encoded['Exited']\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TB-Wgj-iwuj0"
   },
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rH-0IXTVwvgK"
   },
   "outputs": [],
   "source": [
    "# normalizing features using MinMax\n",
    "scaler = MinMaxScaler()\n",
    "train_X = scaler.fit_transform(train_X)\n",
    "test_X = scaler.transform(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dsdZghLtzkR5"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xsLEhQ9qzk8G"
   },
   "source": [
    "# 2. **Clustring**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mk2WoPYoztcD"
   },
   "source": [
    "# Standardizing the numerical feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jo1zxJ1CzlX9"
   },
   "outputs": [],
   "source": [
    "numerical_features = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary']\n",
    "\n",
    "# Standardizing the data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(bank_df_encoded[numerical_features])\n",
    "\n",
    "# Appling K-means clustering\n",
    "kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "bank_df_encoded['Cluster'] = kmeans.fit_predict(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cHepXtkV1N_P"
   },
   "source": [
    "# Visualizing the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YTSLpb7M1OQI",
    "outputId": "f1e00e86-6db5-4ee9-f974-035b2271637b"
   },
   "outputs": [],
   "source": [
    "# Reducing dimensions to 2 using PCA for visualization\n",
    "pca = PCA(n_components=2)\n",
    "pca_components = pca.fit_transform(scaled_data)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(pca_components[:, 0], pca_components[:, 1], c=bank_df_encoded['Cluster'], cmap='viridis')\n",
    "plt.title(\"K-means Clustering of Data\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.colorbar(label='Cluster Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SlGuV92c1nd7",
    "outputId": "172dcef8-8e01-4825-ef87-3b2a7fbd8e24"
   },
   "outputs": [],
   "source": [
    "cluster_summary = bank_df_encoded.groupby('Cluster')[numerical_features].mean()\n",
    "print(cluster_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3ss2Lqq17eq"
   },
   "source": [
    "- **Observations:**\n",
    "\n",
    "1. Cluster 1 and Cluster 3 have high balances and relatively high credit card ownership but differ in age and tenure, Cluster 1 has more young customers with longer tenure while Cluster 3 has younger customers and shorter tenure.\n",
    "\n",
    "2. Cluster 0 seems to represent a group with a moderate data: a normal credit score, moderate balance, and average activity with a reasonable number of products.\n",
    "\n",
    "3. Cluster 2 is the most distinct with very low activity and credit card ownership these customers tend to have high balances but relatively low product usage. The low IsActiveMember value suggests they may be more d less engaged to the bank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "taTHYy222NQs"
   },
   "source": [
    "# Including 'Cluster' as a feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iRD1zUiL2Ndc"
   },
   "outputs": [],
   "source": [
    "X = bank_df_encoded.drop(columns=['Cluster', 'Exited'])\n",
    "y = bank_df_encoded['Exited']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CzcUYJeY2fgH"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O5zP89CVKlac"
   },
   "source": [
    "# 3. **individual and ensemble models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5PLqR-c35Avh"
   },
   "source": [
    "- # **Individual Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-vRfIxd2g0S"
   },
   "source": [
    "# 1. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u0XA4UUx2gCE",
    "outputId": "6492b97e-01ae-406f-e460-9d09651938d2"
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=rf.classes_, yticklabels=rf.classes_)\n",
    "plt.title(\"Confusion Matrix - Random Forest\")\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nsixcxDRT_K3"
   },
   "source": [
    "# roc curve for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EfsgYhSFOq9V",
    "outputId": "a5bf37db-5b06-42dc-fbb3-3cddb1d8ffb5"
   },
   "outputs": [],
   "source": [
    "# Get predicted probabilities for the positive class\n",
    "y_pred_prob = rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr, tpr, label='Random Forest', color=\"r\")\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Random Forest ROC Curve', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ekkxxyv4Z6Z"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Ij7Py244FyI"
   },
   "source": [
    "# 2. Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fzj8eBsG4GCR",
    "outputId": "b1118afb-4f99-4620-b0e9-3164312dec8a"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "params = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf']\n",
    "}\n",
    "\n",
    "# Setting up the GridSearchCV with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(estimator=SVC(), param_grid=params, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=best_model.classes_, yticklabels=best_model.classes_)\n",
    "plt.title(\"Confusion Matrix - SVM (Best Model)\")\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ng-QFSF84GSr"
   },
   "source": [
    "# 3. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TzfqDNIe4Gi2",
    "outputId": "5ffa2556-92bf-4e3a-a70c-e2753658fe21"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "lr_pred = lr_model.predict(X_test_scaled)\n",
    "\n",
    "print(\"Logistic Regression Accuracy: \", accuracy_score(y_test, lr_pred))\n",
    "print(\"Logistic Regression Classification Report: \\n\", classification_report(y_test, lr_pred))\n",
    "conf_matrix = confusion_matrix(y_test, lr_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Greens\", xticklabels=lr_model.classes_, yticklabels=lr_model.classes_)\n",
    "plt.title(\"Confusion Matrix - Logistic Regression\")\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8LEGljBuT453"
   },
   "source": [
    "# roc curve for Logestic Reggression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K9wd8PTcN4Yz",
    "outputId": "5f1dbd28-c065-43e4-c0eb-eab89667dc75"
   },
   "outputs": [],
   "source": [
    "# Get predicted probabilities for the positive class\n",
    "y_pred_prob = lr_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Compute ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr, tpr, label='Logistic Regression', color=\"g\")\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Logistic Regression ROC Curve', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w2CWZFWtEJkK"
   },
   "source": [
    "# Comparison:\n",
    "\n",
    "- Accuracy: SVM has the highest accuracy (94%), followed by random forest (93%) and logistic regression (92%).\n",
    "\n",
    "- Precision: Random Forest has the highest precision for Class 0 (0.96), but SVM performs best for Class 1 (0.83) which indicates the best classification for predicting the customers who leaves. logistic regression precision for Class 1 is the lowest (0.78) and scince my main goal is to predict who leaves the bank this model can't be chosen.\n",
    "\n",
    "- Recall: SVM is the best in recall for Class 1 (0.87), while Random Forest and Logistic Regression are both strong for Class 0 (around 0.96), so random forest is the best as class 1 is more important than class 0.\n",
    "\n",
    "- Logistic Regression has the lowest recall for Class 0 (0.94).\n",
    "\n",
    "- F1-Score: SVM and random forest provide similar F1-scores for both classes, with SVM showing a slightly better overall balance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_JerYdi44WC"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Abgud9-E45Hi"
   },
   "source": [
    "# **Ensamble Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rz_8yYt-4G6B"
   },
   "source": [
    "# 1. Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hYkdSmzZ4HOk",
    "outputId": "c4547a94-8035-4648-b4fd-7101d2873fd1"
   },
   "outputs": [],
   "source": [
    "# Combine individual models in a voting ensemble\n",
    "voting_model = VotingClassifier(estimators=[\n",
    "    ('svm', best_model),\n",
    "    ('lr', lr_model),\n",
    "    ('rf', rf)\n",
    "], voting='hard')  # 'hard' for majority voting, 'soft' for averaging probabilities\n",
    "\n",
    "voting_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "voting_pred = voting_model.predict(X_test_scaled)\n",
    "print(\"Voting Classifier Accuracy: \", accuracy_score(y_test, voting_pred))\n",
    "print(\"Voting Classifier Classification Report: \\n\", classification_report(y_test, voting_pred))\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, voting_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=voting_model.classes_, yticklabels=voting_model.classes_)\n",
    "plt.title(\"Confusion Matrix - Voting Classifier\")\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YLdY6N984HcI"
   },
   "source": [
    "# 2. Bagging (Using Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GfWEn4eG4ISy",
    "outputId": "4dae1524-fe7a-42ef-ba9e-2b8375e0824f"
   },
   "outputs": [],
   "source": [
    "# Train Bagging model using Random Forest\n",
    "bagging_model = BaggingClassifier(estimator=RandomForestClassifier(), n_estimators=50, random_state=42)\n",
    "bagging_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "bagging_pred = bagging_model.predict(X_test)\n",
    "print(\"Bagging Model Accuracy: \", accuracy_score(y_test, bagging_pred))\n",
    "print(\"Bagging Model Classification Report: \\n\", classification_report(y_test, bagging_pred))\n",
    "conf_matrix = confusion_matrix(y_test, bagging_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=bagging_model.classes_, yticklabels=bagging_model.classes_)\n",
    "plt.title(\"Confusion Matrix - Bagging Model\", fontsize=16)\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZPhhA6WTtG5"
   },
   "source": [
    "# roc curve for Bagging (Using Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N-Z_VFwpSdZu",
    "outputId": "aa8e4f21-c72a-4836-8271-bb4885ef42ea"
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr, tpr, label='Bagging Model (RF Estimator)', color=\"b\")\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Bagging Model ROC Curve', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pyf2-41q41uE"
   },
   "source": [
    "# 3. Boosting (Using Gradient Boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b_qb4Bhj42A0",
    "outputId": "67ca8a3e-1d85-4317-de89-bea811366fa2"
   },
   "outputs": [],
   "source": [
    "gb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "gb_pred = gb_model.predict(X_test)\n",
    "print(\"Gradient Boosting Accuracy: \", accuracy_score(y_test, gb_pred))\n",
    "print(\"Gradient Boosting Classification Report: \\n\", classification_report(y_test, gb_pred))\n",
    "conf_matrix = confusion_matrix(y_test, gb_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=gb_model.classes_, yticklabels=gb_model.classes_)\n",
    "plt.title(\"Confusion Matrix - Gradient Boosting\", fontsize=16)\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2TJYDbENTnLK"
   },
   "source": [
    "# roc curve for Boosting (Using Gradient Boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "szuirntATRJm",
    "outputId": "1ebc8dba-5f43-454b-b94f-c635a43f74c8"
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr, tpr, label='Gradient Boosting', color=\"m\")\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Gradient Boosting ROC Curve', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7yU1gso0FqIW"
   },
   "source": [
    "- **Bagging Classifier** (with Random Forest) scored the highest accuracy (93.86%) and has a balanced performance for both classes.\n",
    "\n",
    "- **Gradient Boosting and Voting Classifier** scored similar results with slightly lower accuracy(93.73) than Bagging.\n",
    "\n",
    "- The recall for Class 1 (positive class) is consistently high across all models, indicating good detection of positive instances and the percestion is slightly higher for Bagging and Gradient Boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VClFJvZr5SsO"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EvQC7AcH42eP"
   },
   "source": [
    "# **Comparing Individual vs Ensemble Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WBh-qzjq5alb"
   },
   "source": [
    "# Comparison of Individual vs Ensemble Models\n",
    "\n",
    "- **Note** The tables were generated using GPT, while all analyses were independently written manually.\n",
    "\n",
    "## **1. Individual Models Performance**\n",
    "\n",
    "| **Model**              | **Accuracy** | **Precision (Class 0)** | **Precision (Class 1)** | **Recall (Class 0)** | **Recall (Class 1)** | **F1-Score (Class 0)** | **F1-Score (Class 1)** |\n",
    "|------------------------|--------------|-------------------------|-------------------------|----------------------|----------------------|------------------------|------------------------|\n",
    "| **Random Forest**       | 93.0%        | 0.96                    | 0.84                    | 0.96                 | 0.84                 | 0.96                   | 0.84                   |\n",
    "| **SVM (Best Model)**    | 94.0%        | 0.97                    | 0.83                    | 0.96                 | 0.87                 | 0.96                   | 0.85                   |\n",
    "| **Logistic Regression** | 92.1%        | 0.96                    | 0.78                    | 0.94                 | 0.86                 | 0.95                   | 0.82                   |\n",
    "\n",
    "## **2. Ensemble Models Performance**\n",
    "\n",
    "| **Model**               | **Accuracy** | **Precision (Class 0)** | **Precision (Class 1)** | **Recall (Class 0)** | **Recall (Class 1)** | **F1-Score (Class 0)** | **F1-Score (Class 1)** |\n",
    "|-------------------------|--------------|-------------------------|-------------------------|----------------------|----------------------|------------------------|------------------------|\n",
    "| **Voting Classifier**    | 93.3%        | 0.96                    | 0.82                    | 0.95                 | 0.86                 | 0.96                   | 0.84                   |\n",
    "| **Bagging (Random Forest)** | 93.9%      | 0.97                    | 0.84                    | 0.96                 | 0.87                 | 0.96                   | 0.85                   |\n",
    "| **Gradient Boosting**    | 93.7%        | 0.96                    | 0.84                    | 0.96                 | 0.86                 | 0.96                   | 0.85                   |\n",
    "\n",
    "## **Key Metrics Breakdown**\n",
    "\n",
    "### **1. Accuracy**\n",
    "- **Ensemble Models** (bagging, gradient boosting, and voting) showed a slight improvement over **Individual Models** in accuracy.\n",
    "  - **Bagging** scored the highest accuracy (**93.9%**) followed by **Gradient Boosting** (**93.7%**) and **Voting Classifier** (**93.3%**).\n",
    "  - **SVM** scored the highest individual model accuracy (**94.0%**) while **Random Forest** and **Logistic Regression** were slightly behind so those two will be eliminated from choosing the final model.\n",
    "\n",
    "### **2. Precision**\n",
    "  - **Class 0 (Negative)**: All models perform well, with **Bagging** achieving the highest precision for Class 0 (**0.97**).\n",
    "  - **Class 1 (Positive)**: **Bagging** and **Gradient Boosting** performed the best for Class 1 with precision of **0.84**, while **SVM** is slightly lower at **0.83**. and since percestion for class 1 is what i foucus on bagging and gradient boosting is the two i will be choosing one of them to comment on in terms of bias and variance.\n",
    "\n",
    "### **3. Recall**\n",
    "  - **Class 0**: Models like **Random Forest**, **SVM**, and **Gradient Boosting** are effective in detecting negatives, all with recall values close to **0.96**.\n",
    "  - **Class 1**: **SVM** and **Bagging** had the best recall for Class 1, both at **0.87**, which is slightly better than the rest. **Logistic Regression** has a lower recall of **0.86** for Class 1.\n",
    "\n",
    "### **4. F1-Score**\n",
    "  - **Class 0**: All models have very similar performance for Class 0 with **0.96** F1-Score.\n",
    "  - **Class 1**: **SVM** **Bagging** and **Gradient Boosting** scored the highest F1-Scores for Class 1, all at **0.85**, while **Logistic Regression** is slightly lower at **0.82**.\n",
    "\n",
    "\n",
    "## **Conclusion**\n",
    "\n",
    "- **Ensemble Models** like **Bagging** and **Gradient Boosting** provide slight improvements in **recall** and **precision** for **Class 1 (positive class)** making them better at detecting positive cases.\n",
    "- **Bagging** performed the best overall, with the highest accuracy and precision for **Class 0** and the best recall for **Class 1**.\n",
    "- **SVM** performed best in terms of **accuracy** (**94.0%**), but the ensemble models provide a better balance between **precision**, **recall** and **F1-Score** for both classes.\n",
    "\n",
    "In summary as my aim is class 1 i will be choosing Bagging as it has the highst accuracy and and best f1 score to comment on its Bais and Variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jLUAXJO25jL9"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wyS7Oon7KZIV"
   },
   "source": [
    "# 4. **bias / variance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q5Cmbdtz5ist"
   },
   "source": [
    "## Bagging for Bias-Variance Analysis\n",
    "\n",
    "For the **bias-variance analysis**, I will focus on the **Bagging model** due to the following reasons:\n",
    "\n",
    "- **Balanced Performance**: Based on the performance metrics bagging had shown a good balance between **accuracy** **precision** **recall** and **F1-score** across different classes. Its performance, especially on Class 1 (positive class), is competitive making it an ideal to explore bias and variance.\n",
    "\n",
    "- **Bagging's ability to stabilize predictions** and reduce overfitting by averaging predictions from multiple models makes it a great choice for evaluating bias and variance.\n",
    "\n",
    "Given these strengths, **Bagging** is the most suitable model to evaluate bias and variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-WHwqRwqLWns"
   },
   "source": [
    "## **Bias-Variance Analysis: Bagging Model**\n",
    "\n",
    "### 1. Bagging Model Performance on Training Data:\n",
    "- **Accuracy**: 94.0%\n",
    "- **Precision (Class 0)**: 0.97\n",
    "- **Precision (Class 1)**: 0.84\n",
    "- **Recall (Class 0)**: 0.96\n",
    "- **Recall (Class 1)**: 0.87\n",
    "- **F1-Score (Class 0)**: 0.96\n",
    "- **F1-Score (Class 1)**: 0.85\n",
    "\n",
    "### 2. Bagging Model Performance on Test Data:\n",
    "- **Accuracy**: 93.9%\n",
    "- **Precision (Class 0)**: 0.97\n",
    "- **Precision (Class 1)**: 0.84\n",
    "- **Recall (Class 0)**: 0.96\n",
    "- **Recall (Class 1)**: 0.87\n",
    "- **F1-Score (Class 0)**: 0.96\n",
    "- **F1-Score (Class 1)**: 0.85\n",
    "\n",
    "### Bias-Variance Analysis:\n",
    "\n",
    "#### Training vs Test Performance:\n",
    "- The performance on both the **training** and **test datasets** isalmost identicall.\n",
    "\n",
    "#### Interpretation:\n",
    "- The mini gap between training and test performances suggests that the Bagging model is **not overfitting** to the training data. therefore it has **low variance**.\n",
    "- The performance on both datasets is also good, with **high precision**, **recall**, and **F1-scores** since both metrics are good it indicates that the model is **generalizing well** indicating **low bias** as well.\n",
    "\n",
    "### Conclusion:\n",
    "The **Bagging model** appears to have **low bias** and **low variance** based on the performance metrics. it shows no signs of **underfitting** or **overfitting** meaning the model is well-tuned.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96s3NeIzMAms"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_rm5JZdMBqX"
   },
   "source": [
    "# 5. **Imbalance handling and cost-sensitive classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7zbwLW1u5i7B",
    "outputId": "454fabe3-260b-40e5-d790-119ec8865386"
   },
   "outputs": [],
   "source": [
    "# Check the distribution of classes in the training set\n",
    "print(pd.Series(y_train).value_counts())\n",
    "print(pd.Series(y_test).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MvWnUsk9MK50"
   },
   "source": [
    "- **Based on the class distribution, it seems that the dataset is imbalanced. In both the training and test datasets.**\n",
    "\n",
    "### Percentage of Class 0 and Class 1 in Training Data:\n",
    "\n",
    "- **Percentage of Class 0 in Training Data**:\n",
    "\n",
    " = 79.7\\%\n",
    "\n",
    "\n",
    "- **Percentage of Class 1 in Training Data**:\n",
    "\n",
    " =20.3\\%\n",
    "\n",
    "\n",
    "### Percentage of Class 0 and Class 1 in Testing Data:\n",
    "\n",
    "- **Percentage of Class 0 in Testing Data**:\n",
    "\n",
    "  = 79.6\\%\n",
    "\n",
    "\n",
    "- **Percentage of Class 1 in Testing Data**:\n",
    "\n",
    "  = 20.4\\%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L4tGmObWMZpk"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kZzKWlH7MaPU"
   },
   "source": [
    "# 1. SMOTE (Synthetic Minority Over-sampling Technique):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sy5TI80YMagO",
    "outputId": "6d4ca516-4b4a-49a9-9615-70d1a1342a80"
   },
   "outputs": [],
   "source": [
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "\n",
    "# Resample the training data\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Check the class distribution after applying SMOTE\n",
    "print(pd.Series(y_train_smote).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EwRmWdkXMgud"
   },
   "source": [
    "# 2. Undersampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b9PvqTYVMg_1",
    "outputId": "ad940883-31c3-4e5a-f2e9-d8d462714a2f"
   },
   "outputs": [],
   "source": [
    "# Initialize RandomUnderSampler\n",
    "undersampler = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
    "\n",
    "X_train_under, y_train_under = undersampler.fit_resample(X_train, y_train)\n",
    "\n",
    "print(pd.Series(y_train_under).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70KTpGYwMhNH"
   },
   "source": [
    "# 3. Class Weights (for RandomForest):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hGbaUSDjMhdA",
    "outputId": "09f54a56-0e09-4726-84e7-d89230a4b349"
   },
   "outputs": [],
   "source": [
    "# Initialize RandomForestClassifier with class weights\n",
    "model_with_weights = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "model_with_weights.fit(X_train, y_train)\n",
    "\n",
    "class_distribution = y_train.value_counts()\n",
    "print(class_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rTy9W95CM5we"
   },
   "source": [
    "# Trainning the Models and Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jDeLoluFM5_m",
    "outputId": "f2a97279-43e4-4632-a935-032c07f6bb3e"
   },
   "outputs": [],
   "source": [
    "# Train a model on the SMOTE resampled data\n",
    "model_smote = RandomForestClassifier(random_state=42)\n",
    "model_smote.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Train a model on the Undersampled data\n",
    "model_under = RandomForestClassifier(random_state=42)\n",
    "model_under.fit(X_train_under, y_train_under)\n",
    "\n",
    "# Predictions for each model on the test data\n",
    "y_pred_smote = model_smote.predict(X_test)\n",
    "y_pred_under = model_under.predict(X_test)\n",
    "y_pred_weights = model_with_weights.predict(X_test)\n",
    "\n",
    "# Confusion matrix for each model\n",
    "cm_smote = confusion_matrix(y_test, y_pred_smote)\n",
    "cm_under = confusion_matrix(y_test, y_pred_under)\n",
    "cm_weights = confusion_matrix(y_test, y_pred_weights)\n",
    "\n",
    "# ROC curve for each model\n",
    "fpr_smote, tpr_smote, _ = roc_curve(y_test, model_smote.predict_proba(X_test)[:, 1])\n",
    "fpr_under, tpr_under, _ = roc_curve(y_test, model_under.predict_proba(X_test)[:, 1])\n",
    "fpr_weights, tpr_weights, _ = roc_curve(y_test, model_with_weights.predict_proba(X_test)[:, 1])\n",
    "\n",
    "# Plotting confusion matrices and ROC curves\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Confusion Matrix Comparison\")\n",
    "plt.plot(cm_smote, label=\"SMOTE\", color='r')\n",
    "plt.plot(cm_under, label=\"Undersampling\", color='g')\n",
    "plt.plot(cm_weights, label=\"Class Weights\", color='b')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"ROC Curve Comparison\")\n",
    "plt.plot(fpr_smote, tpr_smote, label=\"SMOTE\", color='r')\n",
    "plt.plot(fpr_under, tpr_under, label=\"Undersampling\", color='g')\n",
    "plt.plot(fpr_weights, tpr_weights, label=\"Class Weights\", color='b')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zam_c_yTQaep"
   },
   "source": [
    "## Models comparison: SMOTE, Undersampling, and Class Weights\n",
    "\n",
    "### ROC Curve:\n",
    "\n",
    "1. **ROC Curves Comparison**:\n",
    "   - All three models (smote, undersampling, class weights) showes an  increase in the true positive rate up to around 0.8 on the y-axis and almost 0.01 on the x-axis. this idicates that the models are able to distinguish between the classes with increasing precision.\n",
    "\n",
    "   -After the initial rise, the curves level off and move toward the top-right corner of the graph (around 0.2 on the x-axis and 1.0 on the y-axis). This shows that the models are performing well, accurately identifying the minority class (Exited).\n",
    "\n",
    "2. **class weights with the largest area under the curve**:\n",
    "   - The **Class Weights** model has the largest Area Under the Curve idicating that it has the best ability to distinguish between classes across different thresholds.\n",
    "\n",
    "---\n",
    "\n",
    "### Confusion Matrix Interpretation:\n",
    "\n",
    "1. **Confusion Matrix for SMOTE, Undersampling, and Class Weights**:\n",
    "\n",
    "     - **SMOTE**: Lines starting from around **2300** on the y-axis and decreasing as the x-axis approaches 0.8, indicating False Negatives and True Positives.\n",
    "\n",
    "     - **Undersampling**: Lines starting from around **2200** on the y-axis, indicating a higher number of False Positives and True Negatives.\n",
    "\n",
    "     - **Class Weights**: Lines starting from around **2350** on the y-axis, showing the best balance between False Positives and True Positives.\n",
    "\n",
    "   - The confusion matrices show:\n",
    "     - **Class Weights**: The model starts from around **100** on the y-axis, with better performance in handling False Positives and True Positives.\n",
    "\n",
    "     - **SMOTE**: The model starts from around **200** on the y-axis, indicating slightly more False Positives compared to Class Weights.\n",
    "    \n",
    "     - **Undersampling**: The model starts from around **250** on the y-axis, indicating the lowest number of False Positives but a higher number of False Negatives.\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion:\n",
    "\n",
    "- **Class Weights** showed the best performance in ROC curve and the better balance of predictions in the confusion matrix. that means it less missclassify the minor class that led to improved generalization and better handling of the class imbalance.\n",
    "\n",
    "- Both **SMOTE** and **Undersampling** were effective but showed some limitations in handling False Negatives or False Positives.\n",
    "\n",
    "Thus, **Class Weights** is the most effective method for handling the class imbalance in my case, providing the best overall performance when compared to **SMOTE** and **Undersampling**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YNrEN9_NSUMQ"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R8IF5c2bRnEJ"
   },
   "source": [
    "# 6. **Analyzing ALL the obtained results are provided in the report**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 3191230,
     "sourceId": 5536933,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
